{
  
    
        "post0": {
            "title": "Problem Statement",
            "content": "Table of Contents . 1&nbsp;&nbsp;Problem Statement | 2&nbsp;&nbsp;Data Load | 3&nbsp;&nbsp;Transformations3.0.1&nbsp;&nbsp;Training | 3.0.2&nbsp;&nbsp;Test | . | . | 4&nbsp;&nbsp;EDA4.1&nbsp;&nbsp;Training | 4.2&nbsp;&nbsp;Training Set Univariate Analysis | 4.3&nbsp;&nbsp;Multivariate analysis4.3.1&nbsp;&nbsp;Exploring the relationships in greater detail | . | . | 5&nbsp;&nbsp;Testing accuracy on a simple logistic | 6&nbsp;&nbsp;Using Sci-kit learn to build and test performance of different models6.1&nbsp;&nbsp;Train holdout split | 6.2&nbsp;&nbsp;Logistic Regression | 6.3&nbsp;&nbsp;Decision Tree | 6.4&nbsp;&nbsp;Random Forest | 6.5&nbsp;&nbsp;SVM | 6.6&nbsp;&nbsp;Naive Bayes | 6.7&nbsp;&nbsp;XGBoost | . | 7&nbsp;&nbsp;Ensembling the models7.1&nbsp;&nbsp;Predict on test | . | 8&nbsp;&nbsp;Stacking the models8.1&nbsp;&nbsp;Predict on test | . | . import statsmodels.api as sm . import pandas as pd import numpy as np import seaborn as sns import re import matplotlib.pyplot as plt . Data Load . train_data = pd.read_csv(&#39;train.csv&#39;) test_data = pd.read_csv(&#39;test.csv&#39;) . train_data[&#39;Survived&#39;].value_counts() . 0 549 1 342 Name: Survived, dtype: int64 . Transformations . Training . Here we are creating a few variables. Child is created based on a definition of &quot;child&quot; historically. . Has_cabin is created because it seems like a cabin number is assigned only if they have a cabin. . Title_Cleaned is the name of their title- Mr, Mrs., etc. . Has_Age just says whether they have a valid age or not. . titles = train_data[&#39;Name&#39;].str.findall(r&#39;, [ w s]* .&#39;) train_data[&#39;Title&#39;] = [re.findall(&#39;[ w s]*&#39;,x[0])[1] for x in titles] . train_data[&#39;Child&#39;] = train_data[&#39;Age&#39;] &lt;= 14 train_data[&#39;Has_Cabin&#39;] = [1 if pd.isnull(x) != True else 0 for x in train_data[&#39;Cabin&#39;]] train_data_title_grouped = train_data.groupby(&#39;Title&#39;,as_index=False).agg({&#39;Survived&#39;:&#39;mean&#39;,&#39;Age&#39;:&#39;max&#39;,&#39;PassengerId&#39;:&#39;count&#39;}) high_titles = train_data_title_grouped.loc[train_data_title_grouped[&#39;PassengerId&#39;] &gt;= 10][&#39;Title&#39;] train_data[&#39;Title_Cleaned&#39;] = [&#39;Other&#39; if x not in list(high_titles) else x for x in train_data[&#39;Title&#39;]] train_data[&#39;Has_Age&#39;] = ~train_data[&#39;Age&#39;].isnull() . train_data_dummies = pd.get_dummies(data=train_data,columns = [&#39;Pclass&#39;,&#39;Sex&#39;,&#39;Embarked&#39;]) . train_data_dummies.head(1) . PassengerId Survived Name Age SibSp Parch Ticket Fare Cabin Title ... Title_Cleaned Has_Age Pclass_1 Pclass_2 Pclass_3 Sex_female Sex_male Embarked_C Embarked_Q Embarked_S . 0 1 | 0 | Braund, Mr. Owen Harris | 22.0 | 1 | 0 | A/5 21171 | 7.25 | NaN | Mr | ... | Mr | True | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | . 1 rows × 22 columns . train_data_dummies[&#39;Age_Impute&#39;] = [np.mean(train_data_dummies[&#39;Age&#39;]) if pd.isnull(x) else x for x in train_data_dummies[&#39;Age&#39;]] . Test . test_data[&#39;Child&#39;] = test_data[&#39;Age&#39;] &lt;= 14 test_data[&#39;Has_Cabin&#39;] = [1 if pd.isnull(x) != True else 0 for x in test_data[&#39;Cabin&#39;]] titles = test_data[&#39;Name&#39;].str.findall(r&#39;, [ w s]* .&#39;) test_data[&#39;Title&#39;] = [re.findall(&#39;[ w s]*&#39;,x[0])[1] for x in titles] test_data[&#39;Title_Cleaned&#39;] = [&#39;Other&#39; if x not in list(high_titles) else x for x in test_data[&#39;Title&#39;]] . test_data[&#39;Has_Age&#39;] = ~test_data[&#39;Age&#39;].isnull() . test_data_dummies = pd.get_dummies(data=test_data,columns = [&#39;Pclass&#39;,&#39;Sex&#39;,&#39;Embarked&#39;]) . EDA . Training . train_data.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Title Child Has_Cabin Title_Cleaned Has_Age . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | Mr | False | 0 | Mr | True | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | Mrs | False | 1 | Mrs | True | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | Miss | False | 0 | Miss | True | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | Mrs | False | 1 | Mrs | True | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | Mr | False | 0 | Mr | True | . train_data.tail() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Title Child Has_Cabin Title_Cleaned Has_Age . 886 887 | 0 | 2 | Montvila, Rev. Juozas | male | 27.0 | 0 | 0 | 211536 | 13.00 | NaN | S | Rev | False | 0 | Other | True | . 887 888 | 1 | 1 | Graham, Miss. Margaret Edith | female | 19.0 | 0 | 0 | 112053 | 30.00 | B42 | S | Miss | False | 1 | Miss | True | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | female | NaN | 1 | 2 | W./C. 6607 | 23.45 | NaN | S | Miss | False | 0 | Miss | False | . 889 890 | 1 | 1 | Behr, Mr. Karl Howell | male | 26.0 | 0 | 0 | 111369 | 30.00 | C148 | C | Mr | False | 1 | Mr | True | . 890 891 | 0 | 3 | Dooley, Mr. Patrick | male | 32.0 | 0 | 0 | 370376 | 7.75 | NaN | Q | Mr | False | 0 | Mr | True | . train_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object 12 Title 891 non-null object 13 Child 891 non-null bool 14 Has_Cabin 891 non-null int64 15 Title_Cleaned 891 non-null object 16 Has_Age 891 non-null bool dtypes: bool(2), float64(2), int64(6), object(7) memory usage: 106.3+ KB . Datatypes are what I would expect . train_data.isnull().mean() . PassengerId 0.000000 Survived 0.000000 Pclass 0.000000 Name 0.000000 Sex 0.000000 Age 0.198653 SibSp 0.000000 Parch 0.000000 Ticket 0.000000 Fare 0.000000 Cabin 0.771044 Embarked 0.002245 Title 0.000000 Child 0.000000 Has_Cabin 0.000000 Title_Cleaned 0.000000 Has_Age 0.000000 dtype: float64 . looks like 20% of Age has some nulls, as well as 80 % of Cabin. The 2 embarked is weird. . train_data.duplicated().sum() . 0 . sns.histplot(train_data[&#39;Age&#39;].dropna(),kde=True) . &lt;AxesSubplot:xlabel=&#39;Age&#39;, ylabel=&#39;Count&#39;&gt; . Seems like there&#39;s a significant bump in the 0 region for age . Based on https://www.encyclopedia-titanica.org/children-on-titanic/, it looks like 14 is the age cutoff to be considered a child. This could be a field that&#39;s added to the training and test dataset. . [display(train_data[x].value_counts()) if train_data[x].dtype == &#39;object&#39; else np.nan for x in train_data] . Calderhead, Mr. Edward Pennington 1 Johnson, Master. Harold Theodor 1 Morrow, Mr. Thomas Rowan 1 Lang, Mr. Fang 1 Sivic, Mr. Husein 1 .. McNamee, Mr. Neal 1 Kent, Mr. Edward Austin 1 Elias, Mr. Dibo 1 Vande Walle, Mr. Nestor Cyriel 1 Ling, Mr. Lee 1 Name: Name, Length: 891, dtype: int64 . male 577 female 314 Name: Sex, dtype: int64 . 347082 7 1601 7 CA. 2343 7 CA 2144 6 3101295 6 .. 348123 1 13567 1 A/5. 13032 1 349209 1 14311 1 Name: Ticket, Length: 681, dtype: int64 . C23 C25 C27 4 B96 B98 4 G6 4 C22 C26 3 D 3 .. D47 1 B86 1 B42 1 B50 1 D48 1 Name: Cabin, Length: 147, dtype: int64 . S 644 C 168 Q 77 Name: Embarked, dtype: int64 . Mr 517 Miss 182 Mrs 125 Master 40 Dr 7 Rev 6 Mlle 2 Major 2 Col 2 Lady 1 Ms 1 Capt 1 Jonkheer 1 the Countess 1 Sir 1 Don 1 Mme 1 Name: Title, dtype: int64 . Mr 517 Miss 182 Mrs 125 Master 40 Other 27 Name: Title_Cleaned, dtype: int64 . [nan, nan, nan, None, None, nan, nan, nan, None, nan, None, None, None, nan, nan, None, nan] . Looks like Name and Ticket and Cabin are kinda useless in their current state. We know that cabin has nulls, so maybe the nulls means that they weren&#39;t assigned a cabin? How many of these nulls are in test? Looks like a decent number as well. 80% are null. Perhaps we could make a variable that says whether it&#39;s cabin or no cabin . train_data[&#39;Child&#39;] = train_data[&#39;Age&#39;] &lt;= 14 train_data[&#39;Has_Cabin&#39;] = [1 if pd.isnull(x) != True else 0 for x in train_data[&#39;Cabin&#39;]] . I&#39;m wondering if there is a way to use the name data. Looks like there is a way to get their title out from the name using Regex. It seems like it&#39;s always between the comma and the period. It seems like there&#39;s a Mr., Mrs., Master, and Miss . train_data.groupby(&#39;Title&#39;,as_index=False).agg({&#39;Survived&#39;:&#39;mean&#39;,&#39;Age&#39;:&#39;max&#39;,&#39;PassengerId&#39;:&#39;count&#39;}) . Title Survived Age PassengerId . 0 Capt | 0.000000 | 70.0 | 1 | . 1 Col | 0.500000 | 60.0 | 2 | . 2 Don | 0.000000 | 40.0 | 1 | . 3 Dr | 0.428571 | 54.0 | 7 | . 4 Jonkheer | 0.000000 | 38.0 | 1 | . 5 Lady | 1.000000 | 48.0 | 1 | . 6 Major | 0.500000 | 52.0 | 2 | . 7 Master | 0.575000 | 12.0 | 40 | . 8 Miss | 0.697802 | 63.0 | 182 | . 9 Mlle | 1.000000 | 24.0 | 2 | . 10 Mme | 1.000000 | 24.0 | 1 | . 11 Mr | 0.156673 | 80.0 | 517 | . 12 Mrs | 0.792000 | 63.0 | 125 | . 13 Ms | 1.000000 | 28.0 | 1 | . 14 Rev | 0.000000 | 57.0 | 6 | . 15 Sir | 1.000000 | 49.0 | 1 | . 16 the Countess | 1.000000 | 33.0 | 1 | . Looking at the title list, it might not be all that helpful. Mrs. and Miss definitely can tell us if they&#39;re married, and Master seems to only be counted for males under 12? Maybe this means that children were only considered under 12? Also, I think I will make all of the &lt;10 ones &#39;Other&#39;. Hmm maybe there&#39;s some help in last names too . train_data_title_grouped = train_data.groupby(&#39;Title&#39;,as_index=False).agg({&#39;Survived&#39;:&#39;mean&#39;,&#39;Age&#39;:&#39;max&#39;,&#39;PassengerId&#39;:&#39;count&#39;}) high_titles = train_data_title_grouped.loc[train_data_title_grouped[&#39;PassengerId&#39;] &gt;= 10][&#39;Title&#39;] train_data[&#39;Title_Cleaned&#39;] = [&#39;Other&#39; if x not in list(high_titles) else x for x in train_data[&#39;Title&#39;]] . Training Set Univariate Analysis . train_data.describe() . PassengerId Survived Pclass Age SibSp Parch Fare Has_Cabin . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | 0.228956 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | 0.420397 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | 0.000000 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | 0.000000 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | 0.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | 1.000000 | . Everything looks fine. I&#39;ll probably make Pclass a dummy variable. Also should be noted that the average &quot;Survived&quot; is 38% . train_data[&#39;Pclass&#39;].value_counts() . 3 491 1 216 2 184 Name: Pclass, dtype: int64 . It looks like most people were poor. Surprisingly the upper class outnumber the middle class. . for x in train_data.select_dtypes(&#39;float&#39;): sns.histplot(x=x,data=train_data) plt.show() . sns.boxplot(train_data[&#39;Age&#39;]) . C: ProgramData Anaconda3 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . &lt;AxesSubplot:xlabel=&#39;Age&#39;&gt; . I want to understand why the 0 bin seemingly has more people relative to what I would understand people aged to be at 0. Is this a lot of newborns? . train_data.loc[train_data[&#39;Age&#39;] &lt;= 5] . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Title Child Has_Cabin Title_Cleaned Has_Age . 7 8 | 0 | 3 | Palsson, Master. Gosta Leonard | male | 2.00 | 3 | 1 | 349909 | 21.0750 | NaN | S | Master | True | 0 | Master | True | . 10 11 | 1 | 3 | Sandstrom, Miss. Marguerite Rut | female | 4.00 | 1 | 1 | PP 9549 | 16.7000 | G6 | S | Miss | True | 1 | Miss | True | . 16 17 | 0 | 3 | Rice, Master. Eugene | male | 2.00 | 4 | 1 | 382652 | 29.1250 | NaN | Q | Master | True | 0 | Master | True | . 43 44 | 1 | 2 | Laroche, Miss. Simonne Marie Anne Andree | female | 3.00 | 1 | 2 | SC/Paris 2123 | 41.5792 | NaN | C | Miss | True | 0 | Miss | True | . 58 59 | 1 | 2 | West, Miss. Constance Mirium | female | 5.00 | 1 | 2 | C.A. 34651 | 27.7500 | NaN | S | Miss | True | 0 | Miss | True | . 63 64 | 0 | 3 | Skoog, Master. Harald | male | 4.00 | 3 | 2 | 347088 | 27.9000 | NaN | S | Master | True | 0 | Master | True | . 78 79 | 1 | 2 | Caldwell, Master. Alden Gates | male | 0.83 | 0 | 2 | 248738 | 29.0000 | NaN | S | Master | True | 0 | Master | True | . 119 120 | 0 | 3 | Andersson, Miss. Ellis Anna Maria | female | 2.00 | 4 | 2 | 347082 | 31.2750 | NaN | S | Miss | True | 0 | Miss | True | . 164 165 | 0 | 3 | Panula, Master. Eino Viljami | male | 1.00 | 4 | 1 | 3101295 | 39.6875 | NaN | S | Master | True | 0 | Master | True | . 171 172 | 0 | 3 | Rice, Master. Arthur | male | 4.00 | 4 | 1 | 382652 | 29.1250 | NaN | Q | Master | True | 0 | Master | True | . 172 173 | 1 | 3 | Johnson, Miss. Eleanor Ileen | female | 1.00 | 1 | 1 | 347742 | 11.1333 | NaN | S | Miss | True | 0 | Miss | True | . 183 184 | 1 | 2 | Becker, Master. Richard F | male | 1.00 | 2 | 1 | 230136 | 39.0000 | F4 | S | Master | True | 1 | Master | True | . 184 185 | 1 | 3 | Kink-Heilmann, Miss. Luise Gretchen | female | 4.00 | 0 | 2 | 315153 | 22.0250 | NaN | S | Miss | True | 0 | Miss | True | . 193 194 | 1 | 2 | Navratil, Master. Michel M | male | 3.00 | 1 | 1 | 230080 | 26.0000 | F2 | S | Master | True | 1 | Master | True | . 205 206 | 0 | 3 | Strom, Miss. Telma Matilda | female | 2.00 | 0 | 1 | 347054 | 10.4625 | G6 | S | Miss | True | 1 | Miss | True | . 233 234 | 1 | 3 | Asplund, Miss. Lillian Gertrud | female | 5.00 | 4 | 2 | 347077 | 31.3875 | NaN | S | Miss | True | 0 | Miss | True | . 261 262 | 1 | 3 | Asplund, Master. Edvin Rojj Felix | male | 3.00 | 4 | 2 | 347077 | 31.3875 | NaN | S | Master | True | 0 | Master | True | . 297 298 | 0 | 1 | Allison, Miss. Helen Loraine | female | 2.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | Miss | True | 1 | Miss | True | . 305 306 | 1 | 1 | Allison, Master. Hudson Trevor | male | 0.92 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | Master | True | 1 | Master | True | . 340 341 | 1 | 2 | Navratil, Master. Edmond Roger | male | 2.00 | 1 | 1 | 230080 | 26.0000 | F2 | S | Master | True | 1 | Master | True | . 348 349 | 1 | 3 | Coutts, Master. William Loch &quot;William&quot; | male | 3.00 | 1 | 1 | C.A. 37671 | 15.9000 | NaN | S | Master | True | 0 | Master | True | . 374 375 | 0 | 3 | Palsson, Miss. Stina Viola | female | 3.00 | 3 | 1 | 349909 | 21.0750 | NaN | S | Miss | True | 0 | Miss | True | . 381 382 | 1 | 3 | Nakid, Miss. Maria (&quot;Mary&quot;) | female | 1.00 | 0 | 2 | 2653 | 15.7417 | NaN | C | Miss | True | 0 | Miss | True | . 386 387 | 0 | 3 | Goodwin, Master. Sidney Leonard | male | 1.00 | 5 | 2 | CA 2144 | 46.9000 | NaN | S | Master | True | 0 | Master | True | . 407 408 | 1 | 2 | Richards, Master. William Rowe | male | 3.00 | 1 | 1 | 29106 | 18.7500 | NaN | S | Master | True | 0 | Master | True | . 445 446 | 1 | 1 | Dodge, Master. Washington | male | 4.00 | 0 | 2 | 33638 | 81.8583 | A34 | S | Master | True | 1 | Master | True | . 448 449 | 1 | 3 | Baclini, Miss. Marie Catherine | female | 5.00 | 2 | 1 | 2666 | 19.2583 | NaN | C | Miss | True | 0 | Miss | True | . 469 470 | 1 | 3 | Baclini, Miss. Helene Barbara | female | 0.75 | 2 | 1 | 2666 | 19.2583 | NaN | C | Miss | True | 0 | Miss | True | . 479 480 | 1 | 3 | Hirvonen, Miss. Hildur E | female | 2.00 | 0 | 1 | 3101298 | 12.2875 | NaN | S | Miss | True | 0 | Miss | True | . 530 531 | 1 | 2 | Quick, Miss. Phyllis May | female | 2.00 | 1 | 1 | 26360 | 26.0000 | NaN | S | Miss | True | 0 | Miss | True | . 618 619 | 1 | 2 | Becker, Miss. Marion Louise | female | 4.00 | 2 | 1 | 230136 | 39.0000 | F4 | S | Miss | True | 1 | Miss | True | . 642 643 | 0 | 3 | Skoog, Miss. Margit Elizabeth | female | 2.00 | 3 | 2 | 347088 | 27.9000 | NaN | S | Miss | True | 0 | Miss | True | . 644 645 | 1 | 3 | Baclini, Miss. Eugenie | female | 0.75 | 2 | 1 | 2666 | 19.2583 | NaN | C | Miss | True | 0 | Miss | True | . 691 692 | 1 | 3 | Karun, Miss. Manca | female | 4.00 | 0 | 1 | 349256 | 13.4167 | NaN | C | Miss | True | 0 | Miss | True | . 750 751 | 1 | 2 | Wells, Miss. Joan | female | 4.00 | 1 | 1 | 29103 | 23.0000 | NaN | S | Miss | True | 0 | Miss | True | . 755 756 | 1 | 2 | Hamalainen, Master. Viljo | male | 0.67 | 1 | 1 | 250649 | 14.5000 | NaN | S | Master | True | 0 | Master | True | . 777 778 | 1 | 3 | Emanuel, Miss. Virginia Ethel | female | 5.00 | 0 | 0 | 364516 | 12.4750 | NaN | S | Miss | True | 0 | Miss | True | . 788 789 | 1 | 3 | Dean, Master. Bertram Vere | male | 1.00 | 1 | 2 | C.A. 2315 | 20.5750 | NaN | S | Master | True | 0 | Master | True | . 803 804 | 1 | 3 | Thomas, Master. Assad Alexander | male | 0.42 | 0 | 1 | 2625 | 8.5167 | NaN | C | Master | True | 0 | Master | True | . 824 825 | 0 | 3 | Panula, Master. Urho Abraham | male | 2.00 | 4 | 1 | 3101295 | 39.6875 | NaN | S | Master | True | 0 | Master | True | . 827 828 | 1 | 2 | Mallet, Master. Andre | male | 1.00 | 0 | 2 | S.C./PARIS 2079 | 37.0042 | NaN | C | Master | True | 0 | Master | True | . 831 832 | 1 | 2 | Richards, Master. George Sibley | male | 0.83 | 1 | 1 | 29106 | 18.7500 | NaN | S | Master | True | 0 | Master | True | . 850 851 | 0 | 3 | Andersson, Master. Sigvard Harald Elias | male | 4.00 | 4 | 2 | 347082 | 31.2750 | NaN | S | Master | True | 0 | Master | True | . 869 870 | 1 | 3 | Johnson, Master. Harold Theodor | male | 4.00 | 1 | 1 | 347742 | 11.1333 | NaN | S | Master | True | 0 | Master | True | . I guess there were just a lot of children on this cruise . for var in [&#39;Survived&#39;,&#39;Pclass&#39;,&#39;Sex&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Embarked&#39;,&#39;Child&#39;,&#39;Has_Cabin&#39;,&#39;Title_Cleaned&#39;,&#39;Has_Age&#39;]: sns.countplot(x=var,data=train_data) plt.show() plt.close() . Multivariate analysis . train_data.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Title Child Has_Cabin Title_Cleaned Has_Age . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | Mr | False | 0 | Mr | True | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | Mrs | False | 1 | Mrs | True | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | Miss | False | 0 | Miss | True | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | Mrs | False | 1 | Mrs | True | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | Mr | False | 0 | Mr | True | . sns.heatmap(train_data.corr(),annot=True, cmap=&quot;coolwarm&quot;,center=0) . &lt;AxesSubplot:&gt; . It looks like there&#39;s significant negative correlation between Pclass and Has Cabin, which in this context means that the people that have cabins probably are higher class. . It also seems like there&#39;s a pretty strong positive correlation between Fare and Has_Cabin, which means that the higher the fare, the more likely they are to have a cabin. . Age and Child are negatively correclated, which makes sense since I created Child as anyone under age 14. . Fare and Pclass are negatively correlated, which means that the less you pay the more likely you are to be a lower social class. . Interestingly, Survived has a decently high correlation with Pclass, Fare, and Has_Cabin. These variables are also highly correlated with each other, so I&#39;m wondering if there will be a lot of collinearity introduced by having all 3 of these in the model. . Also interestingly, it seems that Age has a moderately negative correlation between Pclass and SibSp. That means the younger you are, the more likely you are to be of a higher social class. Also, the older you are, the less likely you are to have siblings or spouses on the ship. (which makes sense, since younger children can have multiple siblings but older people are most likely to only have 1 spouse or no spouse.) . sns.barplot(x=&#39;Sex&#39;,y=&#39;Survived&#39;,data=train_data) . &lt;AxesSubplot:xlabel=&#39;Sex&#39;, ylabel=&#39;Survived&#39;&gt; . Exploring the relationships in greater detail . sns.boxplot(x=train_data[&#39;Survived&#39;],y=train_data[&#39;Age&#39;]) . &lt;AxesSubplot:xlabel=&#39;Survived&#39;, ylabel=&#39;Age&#39;&gt; . Surprisingly, the median age of the people that survived and that didn&#39;t are very similar. . sns.boxplot(x=train_data[&#39;Pclass&#39;],y=train_data[&#39;Age&#39;]) . &lt;AxesSubplot:xlabel=&#39;Pclass&#39;, ylabel=&#39;Age&#39;&gt; . It seems that the age gets lower as the social class gets lower. Meaning younger people tended to be middle or lower class. . sns.boxplot(x=train_data[&#39;SibSp&#39;],y=train_data[&#39;Age&#39;]) . &lt;AxesSubplot:xlabel=&#39;SibSp&#39;, ylabel=&#39;Age&#39;&gt; . The titles I think correspond to the number of each respective gender aboard the titanic. . Testing accuracy on a simple logistic . train_data_dropna = train_data_dummies.dropna() . y = train_data_dropna[[&#39;Survived&#39;]] X = train_data_dropna[[&#39;Age&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Fare&#39;,&#39;Child&#39;,&#39;Has_Cabin&#39;,&#39;Pclass_1&#39;,&#39;Pclass_2&#39;,&#39;Sex_female&#39;,&#39;Embarked_C&#39;,&#39;Embarked_Q&#39;]] X = sm.add_constant(X) logistic = sm.Logit(endog = y.astype(&#39;float&#39;),exog = X.astype(&#39;float&#39;),missing=&#39;drop&#39;) res = logistic.fit() . Optimization terminated successfully. Current function value: 0.429363 Iterations 7 . y = train_data_dummies[[&#39;Survived&#39;]] X = train_data_dummies[[&#39;Age_Impute&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Fare&#39;,&#39;Child&#39;,&#39;Has_Cabin&#39;,&#39;Has_Age&#39;,&#39;Pclass_1&#39;,&#39;Pclass_2&#39;,&#39;Sex_female&#39;,&#39;Embarked_C&#39;,&#39;Embarked_Q&#39;]] X = sm.add_constant(X) logistic1 = sm.Logit(endog = y.astype(&#39;float&#39;),exog = X.astype(&#39;float&#39;),missing=&#39;drop&#39;) res1 = logistic1.fit() . Optimization terminated successfully. Current function value: 0.429244 Iterations 6 . res.summary() . Logit Regression Results Dep. Variable: Survived | No. Observations: 185 | . Model: Logit | Df Residuals: 174 | . Method: MLE | Df Model: 10 | . Date: Sat, 24 Jul 2021 | Pseudo R-squ.: 0.3186 | . Time: 20:30:04 | Log-Likelihood: -79.432 | . converged: True | LL-Null: -116.57 | . Covariance Type: nonrobust | LLR p-value: 6.605e-12 | . | coef std err z P&gt;|z| [0.025 0.975] . Age -0.0310 | 0.016 | -1.903 | 0.057 | -0.063 | 0.001 | . SibSp 0.1284 | 0.358 | 0.359 | 0.720 | -0.573 | 0.830 | . Parch -0.5542 | 0.343 | -1.616 | 0.106 | -1.226 | 0.118 | . Fare 0.0021 | 0.003 | 0.670 | 0.503 | -0.004 | 0.008 | . Child 1.2533 | 1.117 | 1.123 | 0.262 | -0.935 | 3.442 | . Has_Cabin -0.7971 | 1.025 | -0.778 | 0.437 | -2.806 | 1.211 | . Pclass_1 1.5922 | 0.943 | 1.689 | 0.091 | -0.255 | 3.440 | . Pclass_2 1.4364 | 1.160 | 1.239 | 0.215 | -0.836 | 3.709 | . Sex_female 3.0421 | 0.519 | 5.860 | 0.000 | 2.025 | 4.060 | . Embarked_C 0.4119 | 0.450 | 0.915 | 0.360 | -0.471 | 1.294 | . Embarked_Q -1.5026 | 1.977 | -0.760 | 0.447 | -5.378 | 2.373 | . With this initial simple analysis, we see that the most predictive variables are Gender and Age. Since we dropped a lot of columns, this might not be a representative dataset, and none of the others are really that significant. . res1.summary() . Logit Regression Results Dep. Variable: Survived | No. Observations: 891 | . Model: Logit | Df Residuals: 878 | . Method: MLE | Df Model: 12 | . Date: Sat, 24 Jul 2021 | Pseudo R-squ.: 0.3554 | . Time: 20:30:04 | Log-Likelihood: -382.46 | . converged: True | LL-Null: -593.33 | . Covariance Type: nonrobust | LLR p-value: 9.356e-83 | . | coef std err z P&gt;|z| [0.025 0.975] . const -1.8341 | 0.368 | -4.985 | 0.000 | -2.555 | -1.113 | . Age_Impute -0.0226 | 0.009 | -2.424 | 0.015 | -0.041 | -0.004 | . SibSp -0.4313 | 0.125 | -3.455 | 0.001 | -0.676 | -0.187 | . Parch -0.2286 | 0.129 | -1.769 | 0.077 | -0.482 | 0.025 | . Fare 0.0029 | 0.003 | 1.105 | 0.269 | -0.002 | 0.008 | . Child 1.4142 | 0.459 | 3.080 | 0.002 | 0.514 | 2.314 | . Has_Cabin 0.8653 | 0.339 | 2.554 | 0.011 | 0.201 | 1.529 | . Has_Age 0.1066 | 0.259 | 0.411 | 0.681 | -0.401 | 0.615 | . Pclass_1 1.3610 | 0.398 | 3.416 | 0.001 | 0.580 | 2.142 | . Pclass_2 1.1244 | 0.243 | 4.621 | 0.000 | 0.648 | 1.601 | . Sex_female 2.7677 | 0.206 | 13.427 | 0.000 | 2.364 | 3.172 | . Embarked_C 0.4618 | 0.246 | 1.879 | 0.060 | -0.020 | 0.944 | . Embarked_Q 0.4916 | 0.353 | 1.394 | 0.163 | -0.199 | 1.183 | . With the whole dataset and using imputation to create Age, we see that Age and Gender are still very predictive. But now, we see that Pclass is also very predictive. This leads me to believe that the people without age might be biased, in that people with worse Pclass is less likely to have an age. In addition, it seems that being a child or having a cabin really raises your chances of survival. . Using Sci-kit learn to build and test performance of different models . from sklearn.model_selection import train_test_split . from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline . from sklearn.model_selection import cross_validate . from sklearn.metrics import plot_roc_curve . from sklearn.metrics import plot_precision_recall_curve . from sklearn.metrics import plot_confusion_matrix . from sklearn.model_selection import GridSearchCV . Train holdout split . y = train_data_dummies[&#39;Survived&#39;] . X = train_data_dummies[[&#39;Age&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Fare&#39;,&#39;Child&#39;,&#39;Has_Cabin&#39;,&#39;Has_Age&#39;,&#39;Pclass_1&#39;,&#39;Pclass_2&#39;,&#39;Sex_female&#39;,&#39;Embarked_C&#39;,&#39;Embarked_Q&#39;]] . X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=42) . Logistic Regression . from sklearn.linear_model import LogisticRegression . logistic = LogisticRegression() . log_pipe = Pipeline([(&#39;Imputer&#39;,SimpleImputer(missing_values=np.nan,strategy=&#39;mean&#39;)) ,(&#39;Scaler&#39;,StandardScaler()) ,(&#39;Logistic&#39;,logistic) ]) . cross_validate(log_pipe,X_train,y_train,scoring=[&#39;roc_auc&#39;,&#39;precision&#39;,&#39;recall&#39;]) . {&#39;fit_time&#39;: array([0.03151488, 0.01651502, 0.01899934, 0.01899934, 0.03300023]), &#39;score_time&#39;: array([0.0109992 , 0.01251435, 0.01200104, 0.01599979, 0.02100039]), &#39;test_roc_auc&#39;: array([0.86724927, 0.84217645, 0.84545262, 0.81375874, 0.88089226]), &#39;test_precision&#39;: array([0.76923077, 0.78723404, 0.79166667, 0.68181818, 0.82 ]), &#39;test_recall&#39;: array([0.74074074, 0.68518519, 0.71698113, 0.56603774, 0.75925926])} . Seems to performn decently well, with one fold that performs drastically worse than the others . log_pipe.fit(X_train,y_train) . Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;Logistic&#39;, LogisticRegression())]) . plot_roc_curve(log_pipe, X_test, y_test) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x169357ef670&gt; . plot_precision_recall_curve(log_pipe, X_test, y_test) . &lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x169357efd30&gt; . plot_confusion_matrix(log_pipe, X_test, y_test) . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x16935853c40&gt; . The metrics all look pretty good- high AUC, good precision vs. recall tradeoff, and low misclassifications . Decision Tree . from sklearn.tree import DecisionTreeClassifier . dec_tree = DecisionTreeClassifier() . dec_pipe = Pipeline([(&#39;Imputer&#39;,SimpleImputer(missing_values=np.nan,strategy=&#39;mean&#39;)) ,(&#39;Scaler&#39;,StandardScaler()) ,(&#39;DecTree&#39;,dec_tree) ]) . dec_params = [{&#39;DecTree__max_depth&#39;:[3,5,15] ,&#39;DecTree__max_leaf_nodes&#39;:[15,25,35] ,&#39;DecTree__min_samples_leaf&#39;:[11,21,31]} ] . dec_grid_search = GridSearchCV(dec_pipe,dec_params,cv=5, scoring=&#39;roc_auc&#39;) . GridSearchCV? . dec_grid_search.fit(X_train,y_train) . GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;DecTree&#39;, DecisionTreeClassifier())]), param_grid=[{&#39;DecTree__max_depth&#39;: [3, 5, 15], &#39;DecTree__max_leaf_nodes&#39;: [15, 25, 35], &#39;DecTree__min_samples_leaf&#39;: [11, 21, 31]}], scoring=&#39;roc_auc&#39;) . dec_grid_search.best_params_ . {&#39;DecTree__max_depth&#39;: 5, &#39;DecTree__max_leaf_nodes&#39;: 35, &#39;DecTree__min_samples_leaf&#39;: 21} . dec_tree = DecisionTreeClassifier(max_depth=5,max_leaf_nodes=15,min_samples_leaf=21) . dec_pipe = Pipeline([(&#39;Imputer&#39;,SimpleImputer(missing_values=np.nan,strategy=&#39;mean&#39;)) ,(&#39;Scaler&#39;,StandardScaler()) ,(&#39;DecTree&#39;,dec_tree) ]) . cross_validate(dec_pipe,X_train,y_train,scoring=[&#39;roc_auc&#39;,&#39;precision&#39;,&#39;recall&#39;]) . {&#39;fit_time&#39;: array([0.01099825, 0.01199937, 0.01199913, 0.01100063, 0.00899839]), &#39;score_time&#39;: array([0.01000023, 0.0110023 , 0.01300144, 0.00999761, 0.01300097]), &#39;test_roc_auc&#39;: array([0.87307532, 0.84644195, 0.81831673, 0.84354463, 0.8582702 ]), &#39;test_precision&#39;: array([0.87878788, 0.82978723, 0.6440678 , 0.76923077, 0.79591837]), &#39;test_recall&#39;: array([0.53703704, 0.72222222, 0.71698113, 0.56603774, 0.72222222])} . dec_pipe.fit(X_train,y_train) . Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;DecTree&#39;, DecisionTreeClassifier(max_depth=5, max_leaf_nodes=15, min_samples_leaf=21))]) . plot_roc_curve(dec_pipe,X_train,y_train) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x16934bc1df0&gt; . Random Forest . from sklearn.ensemble import RandomForestClassifier . rand_forest = RandomForestClassifier() . forest_pipe = Pipeline([(&#39;Imputer&#39;,SimpleImputer(missing_values=np.nan,strategy=&#39;mean&#39;)) ,(&#39;Scaler&#39;,StandardScaler()) ,(&#39;RandForest&#39;,rand_forest) ]) . forest_params = [{&#39;RandForest__max_depth&#39;:[3,5,15] ,&#39;RandForest__max_leaf_nodes&#39;:[15,25,35] ,&#39;RandForest__min_samples_leaf&#39;:[1,11,21]} ] . forest_grid = GridSearchCV(forest_pipe,forest_params,cv=5,scoring=&#39;roc_auc&#39;) . forest_grid.fit(X_train,y_train) . GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;RandForest&#39;, RandomForestClassifier())]), param_grid=[{&#39;RandForest__max_depth&#39;: [3, 5, 15], &#39;RandForest__max_leaf_nodes&#39;: [15, 25, 35], &#39;RandForest__min_samples_leaf&#39;: [1, 11, 21]}], scoring=&#39;roc_auc&#39;) . forest_grid.best_params_ . {&#39;RandForest__max_depth&#39;: 15, &#39;RandForest__max_leaf_nodes&#39;: 35, &#39;RandForest__min_samples_leaf&#39;: 1} . rand_forest = RandomForestClassifier(max_depth=15,max_leaf_nodes=25,min_samples_leaf=1) forest_pipe = Pipeline([(&#39;Imputer&#39;,SimpleImputer(missing_values=np.nan,strategy=&#39;mean&#39;)) ,(&#39;Scaler&#39;,StandardScaler()) ,(&#39;RandForest&#39;,rand_forest) ]) . cross_validate(forest_pipe,X_train,y_train,scoring=[&#39;roc_auc&#39;,&#39;precision&#39;,&#39;recall&#39;]) . {&#39;fit_time&#39;: array([0.23000121, 0.2149992 , 0.19099879, 0.17899919, 0.20200157]), &#39;score_time&#39;: array([0.03800035, 0.0318737 , 0.0340004 , 0.02900338, 0.03899765]), &#39;test_roc_auc&#39;: array([0.87723679, 0.85216396, 0.83633665, 0.85637057, 0.88888889]), &#39;test_precision&#39;: array([0.82222222, 0.80851064, 0.8 , 0.79069767, 0.80487805]), &#39;test_recall&#39;: array([0.68518519, 0.7037037 , 0.67924528, 0.64150943, 0.61111111])} . forest_pipe.fit(X_train,y_train) . Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;RandForest&#39;, RandomForestClassifier(max_depth=15, max_leaf_nodes=25))]) . plot_roc_curve(forest_pipe,X_test,y_test) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x16935b25700&gt; . SVM . from sklearn.svm import SVC . svc = SVC(probability=True) . svc_pipe = Pipeline([(&#39;Imputer&#39;,SimpleImputer(missing_values=np.nan,strategy=&#39;mean&#39;)) ,(&#39;Scaler&#39;,StandardScaler()) ,(&#39;SVC&#39;,svc) ]) . svc_params = [{&#39;SVC__kernel&#39;:[&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;]}] . svc_grid = GridSearchCV(svc_pipe,svc_params,cv=5,scoring=&#39;roc_auc&#39;) . svc_grid.fit(X_train,y_train) . GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;SVC&#39;, SVC(probability=True))]), param_grid=[{&#39;SVC__kernel&#39;: [&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;]}], scoring=&#39;roc_auc&#39;) . svc_grid.best_params_ . {&#39;SVC__kernel&#39;: &#39;rbf&#39;} . cross_validate(svc_pipe,X_train,y_train,scoring=[&#39;roc_auc&#39;,&#39;precision&#39;,&#39;recall&#39;]) . {&#39;fit_time&#39;: array([0.05699849, 0.04899836, 0.06100059, 0.05599999, 0.04899716]), &#39;score_time&#39;: array([0.01300025, 0.01699877, 0.01299906, 0.01099777, 0.01200175]), &#39;test_roc_auc&#39;: array([0.89076155, 0.83801498, 0.80941276, 0.80421878, 0.88194444]), &#39;test_precision&#39;: array([0.91891892, 0.82978723, 0.79166667, 0.83333333, 0.82 ]), &#39;test_recall&#39;: array([0.62962963, 0.72222222, 0.71698113, 0.56603774, 0.75925926])} . svc_pipe.fit(X_train,y_train) . Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;SVC&#39;, SVC(probability=True))]) . plot_roc_curve(svc_pipe,X_test,y_test) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x16935b27a60&gt; . The SVC doesn&#39;t look too great... . Naive Bayes . from sklearn.naive_bayes import GaussianNB . nb = GaussianNB() . nb_pipe = Pipeline([(&#39;Imputer&#39;,SimpleImputer(missing_values=np.nan,strategy=&#39;mean&#39;)) ,(&#39;Scaler&#39;,StandardScaler()) ,(&#39;NB&#39;,nb) ]) . cross_validate(nb_pipe,X_train,y_train,scoring=[&#39;roc_auc&#39;,&#39;precision&#39;,&#39;recall&#39;]) . {&#39;fit_time&#39;: array([0.00899935, 0.00999951, 0.01299834, 0.0090003 , 0.01200008]), &#39;score_time&#39;: array([0.00999975, 0.00799894, 0.01199818, 0.01199961, 0.01099873]), &#39;test_roc_auc&#39;: array([0.8460258 , 0.81970454, 0.82340471, 0.80761077, 0.74074074]), &#39;test_precision&#39;: array([0.73584906, 0.76470588, 0.70909091, 0.70833333, 0.66666667]), &#39;test_recall&#39;: array([0.72222222, 0.72222222, 0.73584906, 0.64150943, 0.62962963])} . nb_pipe.fit(X_train,y_train) . Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;NB&#39;, GaussianNB())]) . plot_roc_curve(nb_pipe,X_test,y_test) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x16935b7b2e0&gt; . Hmm not as good as the other ones . XGBoost . import xgboost as xgb . xgboost = xgb.XGBClassifier(use_label_encoder=False,eval_metric=&#39;mlogloss&#39;) . xgb_pipe = Pipeline([(&#39;Imputer&#39;,SimpleImputer(missing_values=np.nan,strategy=&#39;mean&#39;)) ,(&#39;Scaler&#39;,StandardScaler()) ,(&#39;XGB&#39;,xgboost) ]) . cross_validate(xgb_pipe,X_train,y_train,cv=5,scoring=[&#39;roc_auc&#39;,&#39;precision&#39;,&#39;recall&#39;]) . {&#39;fit_time&#39;: array([0.14299703, 0.12600017, 0.14504147, 0.12100005, 0.13803458]), &#39;score_time&#39;: array([0.02100205, 0.01799917, 0.01899815, 0.01800036, 0.02000022]), &#39;test_roc_auc&#39;: array([0.84935497, 0.84155223, 0.85859657, 0.86675853, 0.85606061]), &#39;test_precision&#39;: array([0.79166667, 0.71186441, 0.75510204, 0.73076923, 0.79245283]), &#39;test_recall&#39;: array([0.7037037 , 0.77777778, 0.69811321, 0.71698113, 0.77777778])} . xgb_pipe.fit(X_train,y_train) . Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;XGB&#39;, XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, eval_metric=&#39;mlogloss&#39;, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=&#39;&#39;, learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints=&#39;()&#39;, n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=&#39;exact&#39;, use_label_encoder=False, validate_parameters=1, verbosity=None))]) . plot_roc_curve(xgb_pipe,X_test,y_test) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x1693580e070&gt; . Seems pretty decent . Ensembling the models . from sklearn.ensemble import VotingClassifier . voting_clf = VotingClassifier( estimators=[(&#39;lr&#39;, log_pipe) , (&#39;dt&#39;, dec_pipe) , (&#39;rf&#39;, forest_pipe) ,(&#39;nb&#39;,nb_pipe) ,(&#39;xgb&#39;,xgb_pipe) ,(&#39;svc&#39;,svc_pipe) ], voting=&#39;soft&#39;) . cross_validate(voting_clf,X_train,y_train,scoring=[&#39;roc_auc&#39;,&#39;precision&#39;,&#39;recall&#39;]) . {&#39;fit_time&#39;: array([0.46251011, 0.49702907, 0.48057961, 0.46410918, 0.4519999 ]), &#39;score_time&#39;: array([0.08200121, 0.08500147, 0.08100057, 0.07599998, 0.07700014]), &#39;test_roc_auc&#39;: array([0.88951311, 0.85445277, 0.85880856, 0.85912656, 0.88994108]), &#39;test_precision&#39;: array([0.86046512, 0.78431373, 0.79591837, 0.77777778, 0.83673469]), &#39;test_recall&#39;: array([0.68518519, 0.74074074, 0.73584906, 0.66037736, 0.75925926])} . voting_clf.fit(X_train,y_train) . VotingClassifier(estimators=[(&#39;lr&#39;, Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;Logistic&#39;, LogisticRegression())])), (&#39;dt&#39;, Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;DecTree&#39;, DecisionTreeClassifier(max_depth=5, max_leaf_nodes=15, min_samples_leaf=21))])), (&#39;rf&#39;, Pipeline(steps=[(&#39;Imputer&#39;, Simple... monotone_constraints=&#39;()&#39;, n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=&#39;exact&#39;, use_label_encoder=False, validate_parameters=1, verbosity=None))])), (&#39;svc&#39;, Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;SVC&#39;, SVC(probability=True))]))], voting=&#39;soft&#39;) . plot_roc_curve(voting_clf,X_test,y_test) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x1693597d970&gt; . Predict on test . voting_clf.fit(X,y) . VotingClassifier(estimators=[(&#39;lr&#39;, Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;Logistic&#39;, LogisticRegression())])), (&#39;dt&#39;, Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;DecTree&#39;, DecisionTreeClassifier(max_depth=5, max_leaf_nodes=15, min_samples_leaf=21))])), (&#39;rf&#39;, Pipeline(steps=[(&#39;Imputer&#39;, Simple... monotone_constraints=&#39;()&#39;, n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=&#39;exact&#39;, use_label_encoder=False, validate_parameters=1, verbosity=None))])), (&#39;svc&#39;, Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;SVC&#39;, SVC(probability=True))]))], voting=&#39;soft&#39;) . test_data[&#39;Survived&#39;] = voting_clf.predict(test_data_dummies[[&#39;Age&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Fare&#39;,&#39;Child&#39;,&#39;Has_Cabin&#39;,&#39;Has_Age&#39;,&#39;Pclass_1&#39;,&#39;Pclass_2&#39;,&#39;Sex_female&#39;,&#39;Embarked_C&#39;,&#39;Embarked_Q&#39;]]) . test_data[[&#39;PassengerId&#39;,&#39;Survived&#39;]].to_csv(&#39;ensemble_predictions.csv&#39;,index=False) . Stacking the models . from sklearn.ensemble import StackingClassifier . stacking = StackingClassifier(estimators=[(&#39;lr&#39;, log_pipe) , (&#39;dt&#39;, dec_pipe) , (&#39;rf&#39;, forest_pipe) ,(&#39;nb&#39;,nb_pipe) ,(&#39;xgb&#39;,xgb_pipe) ,(&#39;svc&#39;,svc_pipe) ],) . cross_validate(stacking,X_train,y_train,cv=5,scoring=[&#39;roc_auc&#39;,&#39;precision&#39;,&#39;recall&#39;]) . {&#39;fit_time&#39;: array([2.6955452 , 2.609128 , 2.67016697, 2.58047605, 2.56964278]), &#39;score_time&#39;: array([0.07899809, 0.10399961, 0.08005738, 0.07599998, 0.07764006]), &#39;test_roc_auc&#39;: array([0.89429879, 0.85736579, 0.86347255, 0.83835065, 0.89478114]), &#39;test_precision&#39;: array([0.85714286, 0.75925926, 0.76470588, 0.76744186, 0.81632653]), &#39;test_recall&#39;: array([0.66666667, 0.75925926, 0.73584906, 0.62264151, 0.74074074])} . stacking.fit(X_train,y_train) . StackingClassifier(estimators=[(&#39;lr&#39;, Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;Logistic&#39;, LogisticRegression())])), (&#39;dt&#39;, Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;DecTree&#39;, DecisionTreeClassifier(max_depth=5, max_leaf_nodes=15, min_samples_leaf=21))])), (&#39;rf&#39;, Pipeline(steps=[(&#39;Imputer&#39;, Simp... missing=nan, monotone_constraints=&#39;()&#39;, n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=&#39;exact&#39;, use_label_encoder=False, validate_parameters=1, verbosity=None))])), (&#39;svc&#39;, Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;SVC&#39;, SVC(probability=True))]))]) . plot_roc_curve(stacking,X_test,y_test) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x1693edfe850&gt; . Predict on test . stacking.fit(X,y) . StackingClassifier(estimators=[(&#39;lr&#39;, Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;Logistic&#39;, LogisticRegression())])), (&#39;dt&#39;, Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;DecTree&#39;, DecisionTreeClassifier(max_depth=5, max_leaf_nodes=15, min_samples_leaf=21))])), (&#39;rf&#39;, Pipeline(steps=[(&#39;Imputer&#39;, Simp... missing=nan, monotone_constraints=&#39;()&#39;, n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=&#39;exact&#39;, use_label_encoder=False, validate_parameters=1, verbosity=None))])), (&#39;svc&#39;, Pipeline(steps=[(&#39;Imputer&#39;, SimpleImputer()), (&#39;Scaler&#39;, StandardScaler()), (&#39;SVC&#39;, SVC(probability=True))]))]) . test_data[&#39;Survived&#39;] = stacking.predict(test_data_dummies[[&#39;Age&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Fare&#39;,&#39;Child&#39;,&#39;Has_Cabin&#39;,&#39;Has_Age&#39;,&#39;Pclass_1&#39;,&#39;Pclass_2&#39;,&#39;Sex_female&#39;,&#39;Embarked_C&#39;,&#39;Embarked_Q&#39;]]) . test_data[[&#39;PassengerId&#39;,&#39;Survived&#39;]].to_csv(&#39;ensemble_predictions2.csv&#39;,index=False) .",
            "url": "https://www.yichendong.com/2021/07/24/Titanic-Analysis-and-Prediction.html",
            "relUrl": "/2021/07/24/Titanic-Analysis-and-Prediction.html",
            "date": " • Jul 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://www.yichendong.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.yichendong.com/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello! I’m currently a data scientist that focuses on business impact. I have implemented insurance credit models, credit risk models, and valuations models that have increased profits from 5-15%. Currently, I am focused on using data science to drive business strategy, such as launching new products or revising existing ones. .",
          "url": "https://www.yichendong.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.yichendong.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}